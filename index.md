---
layout: default
---

## About Me

<img class="profile-picture" src="ananya_profile.jpg">

I am a first year CS PhD student at [Khoury College of Computer Science](https://www.khoury.northeastern.edu/) at Northeastern University in Boston. I am advised by Prof [Mai ElSherief](https://www.maielsherief.com/). 

Before starting my PhD, I was a Software Engineer with the [Selection Monitoring and Catalog Systems](https://www.amazon.jobs/content/en/teams/e-commerce-foundation/ascs) organization at Amazon in Seattle. Prior to that I was an MS CS student at [Georgia Tech](https://www.gatech.edu/). At Georgia Tech, I was advised by Prof [Srijan Kumar](https://faculty.cc.gatech.edu/~srijan/) in the CLAWS Lab. I also dabble as a Research mentor with [SimPPL](https://simppl.org/).

<div style="display: flex; align-items: flex-start; gap: 2em;">

  <!-- Left column -->
  <div style="flex: 1;">
    <h2>Updates</h2>
    <ul>
      <li><strong>[August 2025]</strong> Two of my papers have been accepted to EMNLP Findings üéâ. More information coming soon!</li>
      <li><strong>[July 2025]</strong> I was selected as a fellow/mentee for <a href="https://www.cambridgeaisafety.org/mars">MARS</a> with <a href="https://www.geodesicresearch.org/">Geodesic Research</a>.</li>
      <li><strong>[December 2024]</strong> Our paper on <a href="https://arxiv.org/abs/2410.20490">evaluating models for cultural robustness in hate speech detection</a> accepted as an oral presentation at SafeGenAI at NeurIPS!</li>
      <li><strong>[September 2024]</strong> Started my PhD at Northeastern University</li>
      <!-- Add the rest of updates -->
    </ul>
  </div>

  <!-- Right column -->
  <div>
    <img class="profile-picture" src="ananya_profile.jpg" style="max-width: 200px; height: auto;">
  </div>

</div>

## Research Interest

I am interested in designing, evaluating, and aligning AI systems to maximize social benefit while minimizing risks. My work sits at the intersection of natural language processing (NLP), AI alignment, AI safety, and the study of online communities, with a focus on how AI can be more empathetic, fair, and socially aware.

1. AI Safety and Personalization

I deeply interested in studying how LLMs behave and can be made safer and more aligned. My past work has investigated the impact of incorporating user personas and contextual information in the model's behaviour, value alignment, and empathetic capabilities [TBD]. I have been able to explore this impact through both [explicit personas of genders](https://arxiv.org/pdf/2311.14788)) as well as the effect of implicit user personas through dialect [(SafeGenAI Oral Presentation @ NeurIPS'24)](https://arxiv.org/abs/2410.20490). Recently, I have been investigating how the addition of personas and contextual signals into large language models can guide their behavior toward safer, value-aligned outputs.

My ongoing work with [Geodesic Research](https://www.geodesicresearch.org/) @ MARS has been exploring different and internal pathways toward AI control, more on that coming soon!

2. Value Based Alignment:

I am interested in studying how we can align models with human values, with a particular focus on maintaining emotional safety. In my recent work (to appear in EMNLP 2025; paper coming soon üëÄ), we found that models are highly influenced by a user‚Äôs context, often resulting in disproportionate variation in their empathetic capabilities. A further deep dive, conducted by Bangzhao Shu in a paper currently under submission, shows that models express emotions in ways that differ systematically from human emotional expression. Building on these findings, I am exploring ways to reduce this divergence and develop methods that make models emotionally safe, ensuring their responses are consistent, empathetic, and aligned with human expectations.

3. Understanding and Analyzing for Social Good Applications:

In the past, I have studied how AI can support healthier online communities. This includes analyzing misinformation on social media, categorizing [hate speech reasoning](https://ananyamalikk.substack.com/p/intent-to-hate), and [identifying dog whistles](https://drive.google.com/file/d/1hYIJjy92jo9VgBmfIY3AZTr-KUbr0dTa/view). These projects aim to create actionable tools and interventions that make digital spaces safer, more inclusive, and resilient, combining technical research with real-world social impact.


## Publications

- Malik, Ananya; Sharma, Kartik; Ng Lynette Hui Xian; Bhatt, Shaily ‚ÄùWho Speaks Matters: Analysing the Influence
of the Speaker‚Äôs Ethnicity on Hate Classification.‚Äù Accepted to NeurIPS SafeGenAI 2024 (Oral Presentation)
- Malik, Ananya. ‚ÄùEvaluating Large Language Models through Gender and Racial Stereotypes.‚Äù arXiv preprint
arXiv:2311.14788 (2023).
- Successive Image Generation from a Single Sentence, Amogh Parab, Ananya Malik, Arish Damania, Arnav Parekhji,
Pranit Bari, ITM Web Conf. 40 03017 (2021), DOI: 10.1051/itmconf/20214003017.
- A.Malik, Y. Javeri, M. Shah, R. Mangrulkar, ‚ÄòImpact Analysis of Covid 19 News Headlines on Global Economy‚Äô,
Cyber-Physical Systems for COVID-19, Elsevier.
- Malik A. Survey paper on applications of generative adversarial networks in the field of social media. Int J Comput
Appl (IJCA). 2020;175(20):13‚Äì18. doi:10.5120/ijca2020920728

## Academic Service and Groups

Groups 

- [CSG (Computation for Social Good) Lab @ NEU](https://www.maielsherief.com/)
- [Geodesic Research](https://www.geodesicresearch.org/)
- [MAIA](https://aialignment.mit.edu/)
- [SimPPL](https://simppl.org/)
- [CLAWS](https://faculty.cc.gatech.edu/~srijan/)

Teaching

- TA for [CS 5200: Database Management Systems](https://www.khoury.northeastern.edu/people/martin-schedlbauer/)
-  TA for [CS 4100: Foundations of AI (Spring 2025)](https://www.khoury.northeastern.edu/home/camato/4100summaryS25.html) with Prof Chris Amato

- TA for CS 3600: Intro to AI, with Prof James Rehg (Spring 2022) and Prof Mark Reidl (Fall 2022)

Talks

- [Slides](https://www.canva.com/design/DAGjgwNOPBE/Z09A59smG8vXAGvgaW7PFA/view?utm_content=DAGjgwNOPBE&utm_campaign=designshare&utm_medium=link2&utm_source=uniquelinks&utlId=h7faf8a8774) of my lecture on Advanced Topics in AI

- Presentation and Slides at SafeGenAI workship at NeurIPS on [Who Speaks Matters: Analysing the Influence of the Speaker‚Äôs Ethnicity on Hate Classification](https://neurips.cc/virtual/2024/workshop/84705#wse-detail-109375)